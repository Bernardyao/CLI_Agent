# CLI_Agent

A lightweight CLI AI assistant built on top of a large language model. Supports both interactive conversations and one-shot analysis via pipes.

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

## âœ¨ Features

- ğŸ¤– **Interactive chat** â€“ natural language interaction with an AI assistant
- ğŸ”„ **Pipe support** â€“ accept input from stdin and exit after one response
- ğŸ“¤ **Clean output** â€“ streaming output with no extra blank lines
- âš¡ **Streaming responses** â€“ tokens are printed in real time
- ğŸ” **Safe configuration** â€“ API key is read from environment variables only
  

## ğŸš€ Quick Start

### Requirements

- Python 3.10+
- An API key for the QNAI-compatible chat completion API

### Installation

1. **Clone the project**

```bash
git clone https://github.com/Bernardyao/CLI_Agent.git
cd CLI_Agent
```

2. **Create a virtual environment (recommended)**

```bash
python -m venv venv
source venv/bin/activate  # Linux/macOS
# or
venv\Scripts\activate   # Windows
```

3. **Configure API key**

```bash
export QNAI_API_KEY="your_api_key_here"  # Linux/macOS
# or on Windows PowerShell
$Env:QNAI_API_KEY="your_api_key_here"
```

> The current implementation only reads the key from environment variables. Do **not** commit your key into `config.py` or any other file.

4. **Install dependencies**

```bash
pip install -r requirements.txt
```

### Usage

#### Interactive mode

```bash
python agent.py
```

Type your question after the `ag>` prompt. To exit, use one of:

- `/exit`
- `/quit`
- `/q`
- `Ctrl+C` or `Ctrl+D` (EOF)

#### Analyze a file in interactive mode

Use the `/file` command followed by a path:

```bash
python agent.py
ag> /file path/to/file.py
```

The agent will read the file content and ask the model to analyze it using the current conversation context.

#### Pipe mode (one-shot analysis)

```bash
echo "Explain this code" | python agent.py
cat file.txt | python agent.py
```

In pipe mode, the agent reads all stdin, prints a single streaming answer, then exits automatically.

## ğŸ“ Project Structure

```text
CLI_Agent/
â”œâ”€â”€ agent.py              # Main entry point
â”œâ”€â”€ config.py             # API configuration
â”œâ”€â”€ modules/              # Core modules
â”‚   â”œâ”€â”€ input_handler.py  # Input and pipe handling
â”‚   â”œâ”€â”€ llm_client.py     # LLM client (streaming + non-streaming)
â”‚   â””â”€â”€ utils.py          # Output helpers
â”œâ”€â”€ .gitignore            # Git ignore rules
â”œâ”€â”€ LICENSE               # MIT license
â””â”€â”€ README.md             # Project documentation
```

## ğŸ› ï¸ Core Behavior

### 1. Input handling

- Detects piped input from stdin (non-TTY)
- Uses standard `input()` for interactive terminals
- Supports explicit exit commands in interactive mode

### 2. Streaming responses

- Uses the chat completion API with `stream=true`
- Prints tokens as they arrive using `safe_pretty_print`

### 3. Configuration

- API key is read from `QNAI_API_KEY`
- Base URL and model are defined in `config.py`

Example `config.py` excerpt:

```python
import os

API_KEY = os.getenv("QNAI_API_KEY")
BASE_URL = "https://api.qnaigc.com/v1/chat/completions"
MODEL = "deepseek/deepseek-v3.1-terminus"
```

## ğŸ”§ Development

### Quick checks

```bash
# Basic output test
python -c "from modules.utils import pretty_print; pretty_print('Hello, World!')"

# Pipe behavior test
echo "test" | python agent.py
```

## ğŸ“ Changelog (excerpt)

- **v2.0.0 (2025-11-25)**
    - Fixed blank output issues
    - Simplified code structure and improved robustness
    - Refined pipe handling and exit behavior
    - Added a more complete `.gitignore`

## ğŸ“„ License

This project is licensed under the MIT License â€“ see [LICENSE](LICENSE) for details.

## ğŸ™ Acknowledgements

- DeepSeek for the underlying model
- The Python ecosystem for libraries and tooling

